{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model for Newsgroups Data\n",
    "\n",
    "For an explanation of the Naive Bayes model, see [our course notes](https://jennselby.github.io/MachineLearningCourseNotes/#naive-bayes).\n",
    "\n",
    "This notebook uses code from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html.\n",
    "\n",
    "## Instructions\n",
    "0. If you haven't already, follow [the setup instructions here](https://jennselby.github.io/MachineLearningCourseNotes/#setting-up-python3) to get all necessary software installed.\n",
    "0. Read through the code in the following sections:\n",
    "  * [Newgroups Data](#Newgroups-Data)\n",
    "  * [Model Training](#Model-Training)\n",
    "  * [Prediction](#Prediction)\n",
    "0. Complete at least one of the following exercises:\n",
    "  * [Exercise Option #1 - Standard Difficulty](#Exercise-Option-#1---Standard-Difficulty)\n",
    "  * [Exercise Option #2 - Advanced Difficulty](#Exercise-Option-#2---Advanced-Difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups # the 20 newgroups set is included in scikit-learn\n",
    "from sklearn.naive_bayes import MultinomialNB # we need this for our Naive Bayes model\n",
    "\n",
    "# These next two are about processing the data. We'll look into this more later in the semester.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newgroups Data\n",
    "\n",
    "Back in the day, [Usenet](https://en.wikipedia.org/wiki/Usenet_newsgroup) was a popular discussion system where people could discuss topics in relevant newsgroups (think Slack channel or subreddit). At some point, someone pulled together messages sent to 20 different newsgroups, to use as [a dataset for doing text processing](http://qwone.com/~jason/20Newsgroups/).\n",
    "\n",
    "We are going to pull out messages from just a few different groups to try out a Naive Bayes model.\n",
    "\n",
    "Examine the newsgroups dictionary, to make sure you understand the dataset.\n",
    "\n",
    "**Note**: If you get an error about SSL certificates, you can fix this with the following:\n",
    "1. In Finder, click on Applications in the list on the left panel\n",
    "1. Double click to go into the Python folder (it will be called something like Python 3.7)\n",
    "1. Double click on the Install Certificates command in that folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which newsgroups we want to download\n",
    "newsgroup_names = ['comp.graphics', 'rec.sport.hockey', 'sci.electronics', 'sci.space']\n",
    "\n",
    "# get the newsgroup data (organized much like the iris data)\n",
    "newsgroups = fetch_20newsgroups(categories=newsgroup_names, shuffle=True, random_state=265)\n",
    "\n",
    "newsgroups.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part does some processing of the data, because the scikit-learn Naive Bayes module is expecting numerical data rather than text data. We will talk more about what this code is doing later in the semester. For now, you can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text into numbers that represent each word (bag of words method)\n",
    "word_vector = CountVectorizer()\n",
    "word_vector_counts = word_vector.fit_transform(newsgroups.data)\n",
    "\n",
    "# Account for the length of the documents:\n",
    "#   get the frequency with which the word occurs instead of the raw number of times\n",
    "term_freq_transformer = TfidfTransformer()\n",
    "term_freq = term_freq_transformer.fit_transform(word_vector_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now we fit the Naive Bayes model to the subset of the 20 newsgroups data that we've pulled out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Naive Bayes model\n",
    "model = MultinomialNB().fit(term_freq, newsgroups.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Let's see how the model does on some (very short) documents that we made up to fit into the specific categories our model is trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "\tThat GPU has amazing performance with a lot of shaders => comp.graphics\n",
      "\tThe player had a wicked slap shot => rec.sport.hockey\n",
      "\tI spent all day yesterday soldering capacitors banks => sci.space\n",
      "\tI spent all day soldering capacitors banks => sci.electronics\n",
      "\tToday I have to solder a capacitor => sci.electronics\n",
      "\tNASA has rovers on Mars => sci.space\n",
      "\tI like space => sci.space\n",
      "\tI play hockey => rec.sport.hockey\n",
      "\tElectronics are so much better than hockey! => rec.sport.hockey\n",
      "\tI think that electronics have a lot of applications to computer graphics => comp.graphics\n",
      "\tI think that computer graphics have a lot of applications to space => comp.graphics\n",
      "\tI think that graphics have a lot of applications to space => sci.space\n",
      "Probabilities:\n",
      "comp.graphics    rec.sport.hockey sci.electronics  sci.space        \n",
      "0.29466149       0.22895149       0.24926344       0.22712357       \n",
      "0.12948055       0.51155698       0.18248712       0.17647535       \n",
      "0.18765395       0.24247134       0.27724837       0.29262634       \n",
      "0.18846864       0.21260331       0.31055764       0.28837041       \n",
      "0.1693832        0.21646308       0.39940056       0.21475316       \n",
      "0.079185633      0.066225915      0.10236622       0.75222223       \n",
      "0.076952265      0.05878837       0.071638789      0.79262058       \n",
      "0.023960861      0.93022537       0.024383423      0.02143035       \n",
      "0.090120475      0.43414383       0.29826386       0.17747184       \n",
      "0.39302368       0.09457227       0.34935678       0.16304728       \n",
      "0.41566347       0.084025567      0.14255538       0.35775558       \n",
      "0.369471         0.088174254      0.13295768       0.40939707       \n"
     ]
    }
   ],
   "source": [
    "# Predict some new fake documents\n",
    "fake_docs = [\n",
    "    'That GPU has amazing performance with a lot of shaders',\n",
    "    'The player had a wicked slap shot',\n",
    "    'I spent all day yesterday soldering capacitors banks',\n",
    "    'I spent all day soldering capacitors banks',\n",
    "    'Today I have to solder a capacitor',\n",
    "    'NASA has rovers on Mars',\n",
    "    'I like space',\n",
    "    'I play hockey',\n",
    "    'Electronics are so much better than hockey!',\n",
    "    'I think that electronics have a lot of applications to computer graphics',\n",
    "    'I think that computer graphics have a lot of applications to space',\n",
    "    'I think that graphics have a lot of applications to space']\n",
    "fake_counts = word_vector.transform(fake_docs)\n",
    "\n",
    "fake_term_freq = term_freq_transformer.transform(fake_counts)\n",
    "\n",
    "predicted = model.predict(fake_term_freq)\n",
    "print('Predictions:')\n",
    "for doc, group in zip(fake_docs, predicted):\n",
    "    print('\\t{0} => {1}'.format(doc, newsgroups.target_names[group]))\n",
    "\n",
    "probabilities = model.predict_proba(fake_term_freq)\n",
    "print('Probabilities:')\n",
    "print(''.join(['{:17}'.format(name) for name in newsgroups.target_names]))\n",
    "for probs in probabilities:\n",
    "    print(''.join(['{:<17.8}'.format(prob) for prob in probs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Option #1 - Standard Difficulty\n",
    "\n",
    "Modify the fake documents and add some new documents of your own. \n",
    "\n",
    "What words in your documents have particularly large effects on the model probabilities? Note that we're not looking for documents that consist of a single word, but for words that, when included or excluded from a document, tend to change the model's output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting thing to note was that, by removing the word \"yesterday\" in sentence three, the model's classification moved from the space group to the electronics group. A possible explanation for this comes from the exercise below, where it seems that the word \"yesterday\" shows up less frequently in the electronics group than in the space group; as such, not seeing it might have made the model less confident that the message belonged to the space group.\n",
    "\n",
    "Words from the title of the various groups have a lot of effect on the model's prediction. Interestingly, however, there does not appear to be an equal influence. For example, in my third addition I used both hockey and electronics in the phrase. While a human would probably assign that sentence to the electronics group, the model assigned it to hockey with quite some confidence, potentially indicating that \"hockey\" has more weight than \"electronics\". The reason for the higher weight on \"hockey\" might be becuase \"hockey\" only appears in the \"hockey\" group (see below), while \"electronics\" could reasonably appear in multiple groups; as such, the model gives more weight to \"hockey\".\n",
    "\n",
    "It was also interesting to note that the phrase \"computer graphics\" is more heavily weighted than just \"graphics\". This is not because it is longer (as I previously thought), but because it is recognizing both the words \"computer\" and \"graphics\" individually, and assigning more weight for that reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Option #2 - Advanced Difficulty\n",
    "\n",
    "Write some code to count up how often the words you found in the exercise above appear in each category in the training dataset. Does this match up with your intuition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide up Newsgroup data by categories\n",
    "newsgroup_groups=[[] for i in range(len(newsgroups.target_names))]\n",
    "\n",
    "for message, index in zip(newsgroups.data, newsgroups.target):\n",
    "    newsgroup_groups[index].append(message)\n",
    "\n",
    "#Transform messages\n",
    "group_word_vector_counts=[word_vector.transform(group) for group in newsgroup_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words to be counted\n",
    "words=['NASA','the', 'hockey', 'electronics', 'space', 'is', 'yesterday']\n",
    "\n",
    "#Makes list of the numeric value assigned to each of the words under word_vector.transform\n",
    "word_values=[word_vector.transform([i]).nonzero()[1][0] for i in words if word_vector.transform([i]).nnz!=0]\n",
    "\n",
    "#Finds how many of each word appears in the data by category\n",
    "counts=[[sum([entry for entry in group_word_vector_count[:,i]])[0,0] for i in word_values] \n",
    "        for group_word_vector_count in group_word_vector_counts]\n",
    "\n",
    "#Flip axis\n",
    "counts=list(zip(*counts))\n",
    "\n",
    "#Add totals\n",
    "counts_totals=[list(word_counts)+[sum(word_counts)] for word_counts in counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comp.graphics</th>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <th>sci.electronics</th>\n",
       "      <th>sci.space</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NASA</th>\n",
       "      <td>75</td>\n",
       "      <td>14</td>\n",
       "      <td>68</td>\n",
       "      <td>737</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>4537</td>\n",
       "      <td>8080</td>\n",
       "      <td>5527</td>\n",
       "      <td>8264</td>\n",
       "      <td>26408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hockey</th>\n",
       "      <td>0</td>\n",
       "      <td>649</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electronics</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>10</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>space</th>\n",
       "      <td>69</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>1400</td>\n",
       "      <td>1496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1751</td>\n",
       "      <td>1422</td>\n",
       "      <td>1738</td>\n",
       "      <td>2068</td>\n",
       "      <td>6979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yesterday</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             comp.graphics  rec.sport.hockey  sci.electronics  sci.space  \\\n",
       "NASA                    75                14               68        737   \n",
       "the                   4537              8080             5527       8264   \n",
       "hockey                   0               649                0          0   \n",
       "electronics              4                 0              103         10   \n",
       "space                   69                 8               19       1400   \n",
       "is                    1751              1422             1738       2068   \n",
       "yesterday                5                16                1         11   \n",
       "\n",
       "             Total  \n",
       "NASA           894  \n",
       "the          26408  \n",
       "hockey         649  \n",
       "electronics    117  \n",
       "space         1496  \n",
       "is            6979  \n",
       "yesterday       33  "
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df=pandas.DataFrame(counts_totals, columns=newsgroup_names+[\"Total\"], index=words)\n",
    "\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above code, words of interest (such as 'space' and 'hockey') show up reasonably frequently in their respective groups ('space' shows up more than half as often as 'is' in the space group, which is really surprising given how common 'is' is). \n",
    "\n",
    "It is worth noting that the number of uses of 'electronics' is somewhat low compared to some of the other keywords, even in its own group. This is likely not because the electronics group is small, as the electronics group is the third largest if you use the usage of the word 'the' as a proxy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
